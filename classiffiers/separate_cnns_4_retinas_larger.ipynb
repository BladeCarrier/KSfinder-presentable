{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jheuristic/yozhik/ksfinder\n",
      "Metadata does not exist. This run might be MUCH longer due to preprocessing the data.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd ..\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "experiment_name = \"baseline.classifier.cnn_4_retinas.vggsmall\"\n",
    "metadata_path = \"data/mined_retinas_full\"\n",
    "data_path = \"./data/sig56/\"\n",
    "\n",
    "reference_df_path = \"data/decays/long_ks_counts_56.csv\"\n",
    "\n",
    "\n",
    "generate_metadata = not os.path.exists(metadata_path)\n",
    "\n",
    "if generate_metadata:\n",
    "    print \"Metadata does not exist. This run might be MUCH longer due to preprocessing the data.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS=\"device=gpu2\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 2: Tesla K40m (CNMeM is disabled, CuDNN 4004)\n"
     ]
    }
   ],
   "source": [
    "#import theano stack\n",
    "%env THEANO_FLAGS=\"device=gpu2\"\n",
    "\n",
    "import lasagne\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from lib.retina_compiled import retinize_events\n",
    "\n",
    "floatX = theano.config.floatX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess/Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing events: 100/31704\n",
      "processing events: 200/31704\n",
      "processing events: 300/31704\n",
      "processing events: 400/31704\n",
      "processing events: 500/31704\n",
      "processing events: 600/31704\n",
      "processing events: 700/31704\n",
      "processing events: 800/31704\n",
      "processing events: 900/31704\n",
      "processing events: 1000/31704\n",
      "processing events: 1100/31704\n",
      "processing events: 1200/31704\n",
      "processing events: 1300/31704\n",
      "processing events: 1400/31704\n",
      "processing events: 1500/31704\n",
      "processing events: 1600/31704\n",
      "processing events: 1700/31704\n",
      "processing events: 1800/31704\n",
      "processing events: 1900/31704\n",
      "processing events: 2000/31704\n",
      "processing events: 2100/31704\n",
      "processing events: 2200/31704\n",
      "processing events: 2300/31704\n",
      "processing events: 2400/31704\n",
      "processing events: 2500/31704\n",
      "processing events: 2600/31704\n",
      "processing events: 2700/31704\n",
      "processing events: 2800/31704\n",
      "processing events: 2900/31704\n",
      "processing events: 3000/31704\n",
      "processing events: 3100/31704\n",
      "processing events: 3200/31704\n",
      "processing events: 3300/31704\n",
      "processing events: 3400/31704\n",
      "processing events: 3500/31704\n",
      "processing events: 3600/31704\n",
      "processing events: 3700/31704\n",
      "processing events: 3800/31704\n",
      "processing events: 3900/31704\n",
      "processing events: 4000/31704\n",
      "processing events: 4100/31704\n",
      "processing events: 4200/31704\n",
      "processing events: 4300/31704\n",
      "processing events: 4400/31704\n",
      "processing events: 4500/31704\n",
      "processing events: 4600/31704\n",
      "processing events: 4700/31704\n",
      "processing events: 4800/31704\n",
      "processing events: 4900/31704\n",
      "processing events: 5000/31704\n",
      "processing events: 5100/31704\n",
      "processing events: 5200/31704\n",
      "processing events: 5300/31704\n",
      "processing events: 5400/31704\n",
      "processing events: 5500/31704\n",
      "processing events: 5600/31704\n",
      "processing events: 5700/31704\n",
      "processing events: 5800/31704\n",
      "processing events: 5900/31704\n",
      "processing events: 6000/31704\n",
      "processing events: 6100/31704\n",
      "processing events: 6200/31704\n",
      "processing events: 6300/31704\n",
      "processing events: 6400/31704\n",
      "processing events: 6500/31704\n",
      "processing events: 6600/31704\n",
      "processing events: 6700/31704\n",
      "processing events: 6800/31704\n",
      "processing events: 6900/31704\n",
      "processing events: 7000/31704\n",
      "processing events: 7100/31704\n",
      "processing events: 7200/31704\n",
      "processing events: 7300/31704\n",
      "processing events: 7400/31704\n",
      "processing events: 7500/31704\n",
      "processing events: 7600/31704\n",
      "processing events: 7700/31704\n",
      "processing events: 7800/31704\n",
      "processing events: 7900/31704\n",
      "processing events: 8000/31704\n",
      "processing events: 8100/31704\n",
      "processing events: 8200/31704\n",
      "processing events: 8300/31704\n",
      "processing events: 8400/31704\n",
      "processing events: 8500/31704\n",
      "processing events: 8600/31704\n",
      "processing events: 8700/31704\n",
      "processing events: 8800/31704\n",
      "processing events: 8900/31704\n",
      "processing events: 9000/31704\n",
      "processing events: 9100/31704\n",
      "processing events: 9200/31704\n",
      "processing events: 9300/31704\n",
      "processing events: 9400/31704\n",
      "processing events: 9500/31704\n",
      "processing events: 9600/31704\n",
      "processing events: 9700/31704\n",
      "processing events: 9800/31704\n",
      "processing events: 9900/31704\n",
      "processing events: 10000/31704\n",
      "processing events: 10100/31704\n"
     ]
    }
   ],
   "source": [
    "from lib.retina_compiled import retina_view\n",
    "\n",
    "retina_images_path = os.path.join(metadata_path,\"retina_images.npy\")\n",
    "answers_path = os.path.join(metadata_path,\"decay_counts.npy\")\n",
    "\n",
    "if generate_metadata:\n",
    "    \n",
    "\n",
    "    #event names\n",
    "    df_ref = pd.DataFrame.from_csv(reference_df_path,index_col=None)\n",
    "\n",
    "    param_df = pd.DataFrame.from_csv(\"./baseline_mined_retina_params.csv\")\n",
    "    param_df.xdim=32\n",
    "    param_df.ydim=32\n",
    "    retinas = [retina_view(*params) for params in param_df.values]\n",
    "\n",
    "    X = retinize_events(df_ref.X_filename.values,\n",
    "                             data_path,retina_views=retinas,\n",
    "                             max_hits_block=15000,report_rate=100)\n",
    "    \n",
    "    y = df_ref.relevant_decay_count.values >0\n",
    "    \n",
    "    os.mkdir(metadata_path)\n",
    "    np.save(retina_images_path,X)\n",
    "    np.save(answers_path, y)\n",
    "    \n",
    "    generate_metadata = False\n",
    "else:\n",
    "    #load metadata\n",
    "    X = np.load(retina_images_path)\n",
    "    y = np.load(answers_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = X.reshape([5000,-1,64,64])\n",
    "\n",
    "X = X.astype(floatX)\n",
    "y = y.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "Xtr, Xts, Ytr,Yts = train_test_split(X,y,test_size=0.25,random_state=1337)\n",
    "\n",
    "print Ytr.shape,Yts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(verbose=True).fit(Xtr.reshape([len(Xtr),-1]),Ytr)\n",
    "Yts_pred = lr.predict_proba(Xts.reshape([len(Xts),-1]))[:,1]\n",
    "Ytr_pred = lr.predict_proba(Xtr.reshape([len(Xtr),-1]))[:,1]\n",
    "print roc_auc_score(Yts,Yts_pred)\n",
    "print roc_auc_score(Ytr,Ytr_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_dim = (None,)+X.shape[1:]\n",
    "\n",
    "retina_dim = (None,1) + X.shape[2:]\n",
    "\n",
    "retina_images = T.tensor4(\"input_images\",\"floatX\")\n",
    "\n",
    "any_interesting_decays = T.ivector(\"mctruith_any_decays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "concatenated_retinas = retina_images.reshape([retina_images.shape[0],1,-1,retina_images.shape[-1]])\n",
    "\n",
    "concatenated_dim = (None,1,np.prod(X.shape[1:-1]),X.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import InputLayer\n",
    "from lasagne.layers import Conv2DLayer as ConvLayer\n",
    "from lasagne.layers import MaxPool2DLayer as PoolLayer\n",
    "from lasagne.layers import DenseLayer, DropoutLayer\n",
    "\n",
    "#nn where each retina is processed with a separate CNN\n",
    "\n",
    "net = {}\n",
    "\n",
    "net['input'] = InputLayer(concatenated_dim,input_var=concatenated_retinas)\n",
    "net['conv1_1'] = ConvLayer(\n",
    "            net['input'], 256, 5, pad=1, flip_filters=False)\n",
    "net['pool1'] = PoolLayer(net['conv1_1'], 3)\n",
    "net['conv2_1'] = ConvLayer(\n",
    "            net['pool1'], 256, 5, pad=1, flip_filters=False)\n",
    "net['pool2'] = PoolLayer(net['conv2_1'], 3)\n",
    "    \n",
    "net['fc6'] = DenseLayer(net[\"pool2\"], num_units=1024)\n",
    "#net['fc6_dropout'] = DropoutLayer(net['fc6'], p=0.1)\n",
    "\n",
    "net['out'] = DenseLayer(net['fc6'], num_units=2, \n",
    "                        nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = lasagne.layers.get_all_params(net[\"out\"],trainable=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_prediction = lasagne.layers.get_output(net[\"out\"])\n",
    "train_loss_ce = lasagne.objectives.categorical_crossentropy(train_prediction,any_interesting_decays).mean()\n",
    "train_accuracy = lasagne.objectives.categorical_accuracy(train_prediction,any_interesting_decays).mean()\n",
    "updates = lasagne.updates.rmsprop(train_loss_ce,weights,learning_rate=0.001) \n",
    "#ik that lr does not matter. I just dont want an explosion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_fun = theano.function([retina_images,any_interesting_decays],[train_loss_ce,train_accuracy,train_prediction[:,1]], updates=updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = lasagne.layers.get_output(net[\"out\"],deterministic=True)\n",
    "loss_ce = lasagne.objectives.categorical_crossentropy(prediction,any_interesting_decays).mean()\n",
    "accuracy = lasagne.objectives.categorical_accuracy(prediction,any_interesting_decays).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eval_fun = theano.function([retina_images,any_interesting_decays], [loss_ce,accuracy,prediction[:,1]])\n",
    "predict_fun = theano.function([retina_images],prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main loop\n",
    "* almost copies the layout of lasagne basic training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False,crop_at=None):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if crop_at == start_idx:\n",
    "            break\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.metrics import roc_auc_score\n",
    "metrics = defaultdict(dict)\n",
    "\n",
    "import time\n",
    "num_epochs = 100\n",
    "batch_size = 50\n",
    "\n",
    "# We iterate over epochs:\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_acc = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    y_pred_batches = []\n",
    "    y_ref_batches = []\n",
    "    for batch in iterate_minibatches(Xtr, Ytr, batch_size, shuffle=True):\n",
    "        batch_ce,batch_acc,pred_batch= train_fun(*batch)\n",
    "        train_err +=batch_ce\n",
    "        train_acc +=batch_acc\n",
    "        train_batches += 1\n",
    "        y_pred_batches.append(pred_batch)\n",
    "        y_ref_batches.append(batch[1])\n",
    "        \n",
    "    y_pred = np.concatenate(y_pred_batches)\n",
    "    y_ref = np.concatenate(y_ref_batches)\n",
    "    train_auc = roc_auc_score(y_ref,y_pred)\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_err = 0\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    y_pred_batches = []\n",
    "    y_ref_batches = []\n",
    "    for batch in iterate_minibatches(Xts, Yts, batch_size, shuffle=False):\n",
    "        err, acc,pred_batch = eval_fun(*batch)\n",
    "        val_err += err\n",
    "        val_acc += acc\n",
    "        val_batches += 1\n",
    "        y_pred_batches.append(pred_batch)\n",
    "        y_ref_batches.append(batch[1])\n",
    "\n",
    "    y_pred = np.concatenate(y_pred_batches)\n",
    "    y_ref = np.concatenate(y_ref_batches)\n",
    "    val_auc = roc_auc_score(y_ref,y_pred)\n",
    "\n",
    "    metrics[\"acc_train\"][epoch] = train_acc\n",
    "    metrics[\"auc_train\"][epoch] = train_auc\n",
    "    metrics[\"acc_test\"][epoch] = val_acc\n",
    "    metrics[\"auc_test\"][epoch] = val_auc\n",
    "    \n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  training accuracy:\\t\\t{:.2f} %\".format(\n",
    "        train_acc / train_batches * 100))\n",
    "    print(\"  training ROC AUC:\\t\\t{:.3f} %\".format(\n",
    "        train_auc))\n",
    "    print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))\n",
    "    print(\"  validation ROC AUC:\\t\\t{:.3f} %\".format(\n",
    "        val_auc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# After training, we compute and print the test error:\n",
    "test_err = 0\n",
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(Xts, Yts, batch_size, shuffle=False):\n",
    "    inputs, targets = batch\n",
    "    err, acc = eval_fun(inputs, targets)\n",
    "    test_err += err\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results:\")\n",
    "print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
