{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jheuristic/yozhik/ksfinder\n",
      "Metadata does not exist. This run might be MUCH longer due to preprocessing the data.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd ..\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "experiment_name = \"baseline.classifier.cnn_4_retinas.vggsmall\"\n",
    "metadata_path = \"data/8retinas\"\n",
    "data_path = \"data/raw\"\n",
    "\n",
    "generate_metadata = not os.path.exists(metadata_path)\n",
    "\n",
    "if generate_metadata:\n",
    "    print \"Metadata does not exist. This run might be MUCH longer due to preprocessing the data.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS=\"device=gpu1\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 1: Tesla K40m (CNMeM is disabled, CuDNN 4004)\n"
     ]
    }
   ],
   "source": [
    "#import theano stack\n",
    "%env THEANO_FLAGS=\"device=gpu1\"\n",
    "\n",
    "import lasagne\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from lib.retina_compiled import retinize_events\n",
    "\n",
    "floatX = theano.config.floatX\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess/Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "retina_images_path = os.path.join(metadata_path,\"retina_images.npy\")\n",
    "answers_path = os.path.join(metadata_path,\"decay_counts.npy\")\n",
    "\n",
    "if generate_metadata:\n",
    "    index_df_path = os.path.join(data_path,\"paths_and_targets.csv\")\n",
    "\n",
    "    #event names\n",
    "    df_ref = pd.DataFrame.from_csv(index_df_path,index_col=None)\n",
    "\n",
    "    from lib.retina_compiled import retina_view\n",
    "    retinas = [\n",
    "        retina_view(0,0,2000,0,0,50**.5,64,64,np.pi/8,np.pi/8), #r_velo\n",
    "        retina_view(1016.31,172.755,12969.2,0.393943,0.0705397,500**.5,64,64,np.pi/3,np.pi/3),#r_ecal\n",
    "        retina_view(0,0,-200,0,0,50**.5,64,64,np.pi/8,np.pi/8), #r_beforeTT\n",
    "        retina_view(0,500,6000,0,0,500**.5,64,64,0.5472,0.5472), #r_middle\n",
    "        retina_view(0,0,12000,0,0,500**.5,64,64,np.pi/3,np.pi/3), #\n",
    "        retina_view(0,0,1000,0,0,500**.5,64,64,np.pi/3,np.pi/3), #\n",
    "        retina_view(-5000,-5000,-2000,500**.5,64,64,np.pi/3,np.pi/3), #\n",
    "        retina_view(0,500,6000,0,0,500**.5,64,64,np.pi/3,np.pi/3), #\n",
    "        ]\n",
    "\n",
    "\n",
    "    X = retinize_events(df_ref.X_filename.values,\n",
    "                             data_path,retina_views=retinas,\n",
    "                             max_hits_block=15000,report_rate=100)\n",
    "    \n",
    "    y = df_ref.relevant_decay_count.values\n",
    "    \n",
    "    os.mkdir(metadata_path)\n",
    "    np.save(retina_images_path,X)\n",
    "    np.save(answers_path, y)\n",
    "    \n",
    "    generate_metadata = False\n",
    "else:\n",
    "    #load metadata\n",
    "    X = np.load(retina_images_path)\n",
    "    y = np.load(answers_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = X.reshape([-1,1,4*64,64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "Xtr, Xts, Ytr,Yts = train_test_split(X,y,test_size=0.25,random_state=1337)\n",
    "\n",
    "print Ytr.shape,Yts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_dim = (None,)+X.shape[1:]\n",
    "\n",
    "retina_images = T.tensor4(\"input_images\",\"floatX\")\n",
    "\n",
    "any_interesting_decays = T.ivector(\"mctruith_n_decays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import InputLayer\n",
    "from lasagne.layers import Conv2DLayer as ConvLayer\n",
    "from lasagne.layers import MaxPool2DLayer as PoolLayer\n",
    "from lasagne.layers import DenseLayer, DropoutLayer\n",
    "\n",
    "#a short version of vgg16: vgg10 so to say\n",
    "net = {}\n",
    "net['input'] = InputLayer(input_dim,input_var=retina_images)\n",
    "net['conv1_1'] = ConvLayer(\n",
    "    net['input'], 64, 3, pad=1, flip_filters=False)\n",
    "net['conv1_2'] = ConvLayer(\n",
    "    net['conv1_1'], 64, 3, pad=1, flip_filters=False)\n",
    "net['pool1'] = PoolLayer(net['conv1_2'], 2)\n",
    "net['conv2_1'] = ConvLayer(\n",
    "    net['pool1'], 128, 3, pad=1, flip_filters=False)\n",
    "net['conv2_2'] = ConvLayer(\n",
    "    net['conv2_1'], 128, 3, pad=1, flip_filters=False)\n",
    "net['pool2'] = PoolLayer(net['conv2_2'], 2)\n",
    "net['conv3_1'] = ConvLayer(\n",
    "    net['pool2'], 256, 3, pad=1, flip_filters=False)\n",
    "net['conv3_2'] = ConvLayer(\n",
    "    net['conv3_1'], 256, 3, pad=1, flip_filters=False)\n",
    "net['conv3_3'] = ConvLayer(\n",
    "    net['conv3_2'], 256, 3, pad=1, flip_filters=False)\n",
    "net['pool3'] = PoolLayer(net['conv3_3'], 2)\n",
    "net['conv4_1'] = ConvLayer(\n",
    "    net['pool3'], 512, 3, pad=1, flip_filters=False)\n",
    "net['conv4_2'] = ConvLayer(\n",
    "    net['conv4_1'], 512, 3, pad=1, flip_filters=False)\n",
    "net['conv4_3'] = ConvLayer(\n",
    "    net['conv4_2'], 512, 3, pad=1, flip_filters=False)\n",
    "net['pool4'] = PoolLayer(net['conv4_3'], 2)\n",
    "\n",
    "net['fc6'] = DenseLayer(net['pool4'], num_units=4096)\n",
    "net['fc6_dropout'] = DropoutLayer(net['fc6'], p=0.5)\n",
    "\n",
    "net['out'] = DenseLayer(net['fc6_dropout'], num_units=2, \n",
    "                        nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = lasagne.layers.get_all_params(net[\"out\"],trainable=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_prediction = lasagne.layers.get_output(net[\"out\"])\n",
    "train_loss_ce = lasagne.objectives.categorical_crossentropy(train_prediction,any_interesting_decays).mean()\n",
    "train_accuracy = lasagne.objectives.categorical_accuracy(train_prediction,any_interesting_decays).mean()\n",
    "updates = lasagne.updates.adadelta(train_loss_ce,weights,learning_rate=0.05) \n",
    "#ik that lr does not matter. I just dont want an explosion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_fun = theano.function([retina_images,any_interesting_decays],[train_loss_ce,train_accuracy], updates=updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = lasagne.layers.get_output(net[\"out\"],deterministic=True)\n",
    "loss_ce = lasagne.objectives.categorical_crossentropy(train_prediction,any_interesting_decays).mean()\n",
    "accuracy = lasagne.objectives.categorical_accuracy(train_prediction,any_interesting_decays).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eval_fun = theano.function([retina_images,any_interesting_decays], [loss_ce,accuracy])\n",
    "predict_fun = theano.function([retina_images],prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main loop\n",
    "* almost copies the layout of lasagne basic training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False,crop_at=None):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if crop_at == start_idx:\n",
    "            break\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "metrics = defaultdict(dict)\n",
    "\n",
    "import time\n",
    "num_epochs = 100\n",
    "batch_size = 50\n",
    "\n",
    "# We iterate over epochs:\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_acc = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(Xtr, Ytr, batch_size, shuffle=True):\n",
    "        batch_ce,batch_acc= train_fun(*batch)\n",
    "        train_err +=batch_ce\n",
    "        train_acc +=batch_acc\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_err = 0\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(Xts, Yts, batch_size, shuffle=False):\n",
    "        err, acc = eval_fun(*batch)\n",
    "        val_err += err\n",
    "        val_acc += acc\n",
    "        val_batches += 1\n",
    "\n",
    "    metrics[\"acc_train\"][epoch] = train_acc\n",
    "    metrics[\"acc_test\"][epoch] = val_acc\n",
    "    \n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  training accuracy:\\t\\t{:.2f} %\".format(\n",
    "        train_acc / train_batches * 100))\n",
    "    print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# After training, we compute and print the test error:\n",
    "test_err = 0\n",
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(Xts, Yts, batch_size, shuffle=False):\n",
    "    inputs, targets = batch\n",
    "    err, acc = eval_fun(inputs, targets)\n",
    "    test_err += err\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results:\")\n",
    "print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# After training, we compute and print the test error:\n",
    "test_err = 0\n",
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(Xtr, Ytr, batch_size, shuffle=False):\n",
    "    inputs, targets = batch\n",
    "    err, acc = eval_fun(inputs, targets)\n",
    "    test_err += err\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results:\")\n",
    "print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
