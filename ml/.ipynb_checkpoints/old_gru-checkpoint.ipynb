{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "from lasagne.layers import MergeLayer,Gate\n",
    "from lasagne import nonlinearities,init\n",
    "from lasagne.utils import unroll_scan\n",
    "theano.config.floatX = 'float32'\n",
    "\n",
    "floatX = theano.config.floatX\n",
    "_shared = lambda name,val,dtype: theano.shared(val.astype(dtype),name,\n",
    "                                               strict = True,allow_downcast=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.sample([1]).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "                 decision_by_hidden, #both have to be deterministic functions\n",
    "                 input_by_decision,\n",
    "        decision_init_factory=lambda shape:T.zeros(shape[0]+[26],dtype='int32'),\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GRULayer(MergeLayer):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, incoming, num_units,\n",
    "                 \n",
    "                 environment,\n",
    "                 \n",
    "                 resetgate=Gate(W_cell=None),\n",
    "                 updategate=Gate(W_cell=None),\n",
    "                 hidden_update=Gate(W_cell=None,\n",
    "                                    nonlinearity=nonlinearities.tanh),\n",
    "                 \n",
    "                 hid_init_factory=init.Constant(0.),\n",
    "                 \n",
    "                 grad_clipping=5.,\n",
    "                 only_return_final=False,\n",
    "                 **kwargs):\n",
    "\n",
    "        incomings = [incoming]\n",
    "\n",
    "        \n",
    "        # Initialize parent layer\n",
    "        super(GRULayer, self).__init__(incomings, **kwargs)\n",
    "\n",
    "        self.num_units = num_units\n",
    "        self.grad_clipping = grad_clipping\n",
    "        self.only_return_final = only_return_final\n",
    "\n",
    "        \n",
    "        #functions\n",
    "        self.env = environment\n",
    "        self.env.on_new_session(lambda seq_len: self.reset_state)\n",
    "\n",
    "        # Retrieve the dimensionality of the incoming layer\n",
    "        input_shape = self.input_shapes[0]\n",
    "\n",
    "        #shared variables\n",
    "        input_shape = [ (i if i is not None else 1) for i in input_shape]\n",
    "        \n",
    "        hidden_zero = self.hid_init_factory(input)\n",
    "        self.hid_init = _shared(name+\".hid_init_shared\",\n",
    "                                ,floatX)\n",
    "\n",
    "        decision_zero = self.env.decision_init_factory(self.num_units)\n",
    "        self.decision_init = _shared(name+\".decision_init_shared\",\n",
    "                                decision_zero,decision_zero.dtype)\n",
    "\n",
    "        #/shared variables\n",
    "            \n",
    "\n",
    "        ###\n",
    "        # Input dimensionality is the output dimensionality of the input layer\n",
    "        num_inputs = np.prod(input_shape[2:])\n",
    "\n",
    "        def add_gate_params(gate, gate_name):\n",
    "            \"\"\" Convenience function for adding layer parameters from a Gate\n",
    "            instance. \"\"\"\n",
    "            return (self.add_param(gate.W_in, (num_inputs, num_units),\n",
    "                                   name=\"W_in_to_{}\".format(gate_name)),\n",
    "                    self.add_param(gate.W_hid, (num_units, num_units),\n",
    "                                   name=\"W_hid_to_{}\".format(gate_name)),\n",
    "                    self.add_param(gate.b, (num_units,),\n",
    "                                   name=\"b_{}\".format(gate_name),\n",
    "                                   regularizable=False),\n",
    "                    gate.nonlinearity)\n",
    "\n",
    "        # Add in all parameters from gates\n",
    "        (self.W_in_to_updategate, self.W_hid_to_updategate, self.b_updategate,\n",
    "         self.nonlinearity_updategate) = add_gate_params(updategate,\n",
    "                                                         'updategate')\n",
    "        (self.W_in_to_resetgate, self.W_hid_to_resetgate, self.b_resetgate,\n",
    "         self.nonlinearity_resetgate) = add_gate_params(resetgate, 'resetgate')\n",
    "\n",
    "        (self.W_in_to_hidden_update, self.W_hid_to_hidden_update,\n",
    "         self.b_hidden_update, self.nonlinearity_hid) = add_gate_params(\n",
    "             hidden_update, 'hidden_update')\n",
    "        \n",
    "    def reset_state(self,input_shape):\n",
    "        \"\"\"causes re-initialization of shared variables that exist within session scope\n",
    "        [like \"last state\", \"last decision\",etc.]\"\"\"\n",
    "        \n",
    "        input_shape = [ (i if i is not None else 1) for i in input_shape]\n",
    "        \n",
    "        hidden_zero = hid_init_factory(input)\n",
    "        self.hid_init.set_value(hidden_zero)\n",
    "\n",
    "        decision_zero = decision_init_factory(self.num_units)\n",
    "        self.env.decision_init.set_value(decision_zero)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def get_output_shape_for(self, input_shapes):\n",
    "        # The shape of the input to this layer will be the first element\n",
    "        # of input_shapes, whether or not a mask input is being used.\n",
    "        input_shape = input_shapes[0]\n",
    "        # When only_return_final is true, the second (sequence step) dimension\n",
    "        # will be flattened\n",
    "        if self.only_return_final:\n",
    "            return input_shape[0], self.num_units\n",
    "        # Otherwise, the shape will be (n_batch, n_steps, num_units)\n",
    "        else:\n",
    "            return input_shape[0], input_shape[1],2, self.num_units\n",
    "\n",
    "    def get_output_for(self, inputs, **kwargs):\n",
    "        \"\"\"\n",
    "        Compute this layer's output function given a symbolic input variable\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : list of theano.TensorType\n",
    "            `inputs[0]` should always be the symbolic input variable. \n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        layer_output : theano.TensorType\n",
    "            Symbolic output variable.\n",
    "        \"\"\"\n",
    "        # Retrieve the layer input\n",
    "        input = inputs[0]\n",
    "        \n",
    "        assert len(inputs) ==1\n",
    "        \n",
    "        # Treat all dimensions after the second as flattened feature dimensions\n",
    "        if input.ndim > 3:\n",
    "            input = T.flatten(input, 3)\n",
    "            \n",
    "            \n",
    "\n",
    "        # Because scan iterates over the first dimension we dimshuffle to\n",
    "        # (n_time_steps, n_batch, n_features)\n",
    "        input = input.dimshuffle(1, 0, 2)\n",
    "        seq_len, num_batch, _ = input.shape\n",
    "\n",
    "        \n",
    "        # Stack input weight matrices into a (num_inputs, 3*num_units)\n",
    "        # matrix, which speeds up computation\n",
    "        W_in_stacked = T.concatenate(\n",
    "            [self.W_in_to_resetgate, self.W_in_to_updategate,\n",
    "             self.W_in_to_hidden_update], axis=1)\n",
    "\n",
    "        # Same for hidden weight matrices\n",
    "        W_hid_stacked = T.concatenate(\n",
    "            [self.W_hid_to_resetgate, self.W_hid_to_updategate,\n",
    "             self.W_hid_to_hidden_update], axis=1)\n",
    "\n",
    "        # Stack gate biases into a (3*num_units) vector\n",
    "        b_stacked = T.concatenate(\n",
    "            [self.b_resetgate, self.b_updategate,\n",
    "             self.b_hidden_update], axis=0)\n",
    "\n",
    "\n",
    "        \n",
    "        # At each call to scan, input_n will be (n_time_steps, 3*num_units).\n",
    "        # We define a slicing function that extract the input to each GRU gate\n",
    "        def slice_w(x, n):\n",
    "            return x[:, n*self.num_units:(n+1)*self.num_units]\n",
    "\n",
    "        \n",
    "        # Create single recurrent computation step function\n",
    "        # hid_previous was the previous state of hidden/output layer.\n",
    "        # decision_previous is self.env.decision_by_hidden(hid_previous)\n",
    "        \n",
    "        def step(hid_previous,decision_previous, *args):\n",
    "            \n",
    "            input_data = self.env.input_by_decision(decision_previous)\n",
    "            \n",
    "            # Compute W_{hr} h_{t - 1}, W_{hu} h_{t - 1}, and W_{hc} h_{t - 1}\n",
    "            hid_input = T.dot(hid_previous, W_hid_stacked)\n",
    "\n",
    "            if self.grad_clipping:\n",
    "                input_n = theano.gradient.grad_clip(\n",
    "                    input_n, -self.grad_clipping, self.grad_clipping)\n",
    "                hid_input = theano.gradient.grad_clip(\n",
    "                    hid_input, -self.grad_clipping, self.grad_clipping)\n",
    "\n",
    "            # Compute W_{xr}x_t + b_r, W_{xu}x_t + b_u, and W_{xc}x_t + b_c\n",
    "            input_n = T.dot(input_data, W_in_stacked) + b_stacked\n",
    "\n",
    "            # Reset and update gates\n",
    "            resetgate = slice_w(hid_input, 0) + slice_w(input_n, 0)\n",
    "            updategate = slice_w(hid_input, 1) + slice_w(input_n, 1)\n",
    "            resetgate = self.nonlinearity_resetgate(resetgate)\n",
    "            updategate = self.nonlinearity_updategate(updategate)\n",
    "\n",
    "            # Compute W_{xc}x_t + r_t \\odot (W_{hc} h_{t - 1})\n",
    "            hidden_update_in = slice_w(input_n, 2)\n",
    "            hidden_update_hid = slice_w(hid_input, 2)\n",
    "            hidden_update = hidden_update_in + resetgate*hidden_update_hid\n",
    "            \n",
    "            if self.grad_clipping:\n",
    "                hidden_update = theano.gradient.grad_clip(\n",
    "                    hidden_update, -self.grad_clipping, self.grad_clipping)\n",
    "            hidden_update = self.nonlinearity_hid(hidden_update)\n",
    "\n",
    "            # Compute (1 - u_t)h_{t - 1} + u_t c_t\n",
    "            hid = (1 - updategate)*hid_previous + updategate*hidden_update\n",
    "            \n",
    "            decision = self.env.decision_by_hidden(hid)\n",
    "\n",
    "            return hid,decision\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        # The hidden-to-hidden weight matrix is always used in step\n",
    "        # As we don't precompute inputs, we have to\n",
    "        # provide the input weights and biases to the step function\n",
    "        non_seqs = [W_hid_stacked,W_in_stacked, b_stacked]\n",
    "        \n",
    "        #output initializers\n",
    "        hid_init = self.hid_init\n",
    "        decision_init = self.decision_init\n",
    "        outputs_info = [hid_init,decision_init]\n",
    "\n",
    "        \n",
    "        # Scan op iterates over first dimension of input and repeatedly\n",
    "        # applies the step function\n",
    "        (dec_out,hid_out) = theano.scan(\n",
    "            fn=step_fun,\n",
    "            #sequences=[],#replaced by n_steps\n",
    "            go_backwards=self.backwards,\n",
    "            outputs_info=outputs_info,\n",
    "            non_sequences=non_seqs,\n",
    "            truncate_gradient=self.gradient_steps,\n",
    "            strict=True,\n",
    "            n_steps = seq_len\n",
    "        )[0]\n",
    "        \n",
    "        hid_out = hid_out.reshape(seq_len,num_batch,1,-1)\n",
    "        dec_out = dec_out.reshape(seq_len,num_batch,1,-1)\n",
    "        \n",
    "        hid_and_dec = T.concatenate([hid_out,dec_out],axis=2)\n",
    "        \n",
    "        # When it is requested that we only return the final sequence step,\n",
    "        # we need to slice it out immediately after scan is applied\n",
    "        if self.only_return_final:\n",
    "            hid_out = hid_out[-1]\n",
    "            dec_out = dec_out[-1]\n",
    "        else:\n",
    "            # dimshuffle back to (n_batch, n_time_steps, n_features))\n",
    "            hid_out = hid_out.dimshuffle(1, 0, 2)\n",
    "\n",
    "            # if scan is backward reverse the output\n",
    "            if self.backwards:\n",
    "                hid_out = hid_out[:, ::-1]\n",
    "\n",
    "        return hid_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(4)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.ones([1,2,3,4]).shape.shape[0].eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
